[
["index.html", "The VY Notebook Notes on my learnings and understandings Preface", " The VY Notebook Notes on my learnings and understandings Vaibhav Yadav 2021-05-14 Preface This book uses Bookdown for all the articles. Bookdown supports live code in R and Python (many other languages as well). The code outputs can be embedded in the book directly. Bookdown also has many other options that help take lecture notes. This is why I started taking notes in this bookdown rather than other MarkDown blogging softwares. This book has notes and understandings of articles and lectures taken by me, Vaibhav Yadav. These articles/chapters might be incomplete or hard to understand as they are not written for explaining any topic but are for dumping the notes to refer back. I however, include various links and references whereever possible, these can be reference when needed. If you have any suggestions, please email author at iYadavVaibhav [at] gmail [dot] com. Thank you! "],
["probability.html", "1 Probability 1.1 Events 1.2 Addition Rule 1.3 Multiplication Rule 1.4 Marginal or Unconditional Probability: 1.5 Joint Probability 1.6 Conditional Probability 1.7 Counting Events 1.8 Approach to solve a problem", " 1 Probability Probability is a way to find likeliness of an event to happen. Permutation and combination are ways to count events and possibilities. Statistics is used to summarize data. It tells us: how likely event is going to happen. possibility of event that is fundamentally random. Quantifying the uncertainity. Any experiment can have outcome for an event. The possibility of outcome is the probability. Experiment -&gt; many events -&gt; outcomes Events make sample space, that is, all possible outcomes. For example, if we flip a coin we can get either heads or tails. The possibility of heads is 50% and possibility ability of tails is 50%. So the probability of Heads is .5 and probability of tails is also .5, if it is a biased coin. The analysis of event governed by probability is called statistics. \\[ P(e) = \\frac{ Possibilities }{ Outcomes } \\] Theoretical or Classical Probability It can be stated and seems fixed. For example flipping a coin. Experimental or Subjective Probability Finding an outcome based on past data and experience example prediction of the score. Probability gives a reasonable predictions about an outcome. It is highly likely but not hundred percent true. Simulation and Randomness We can use list of random numbers to simulate our experiment multiple times and average out to find confidence. 1.1 Events Every possible outcome of a variable is an event. Simple event described by a single characteristic. For eg, a day in January from all days in 2018. Complement of an event A (denoted A‚Äô). All events that are not part of event A. For eg, all days from 2018 that are not in January. Joint event described by two or more characteristics. For eg, a day in January that is also a Wednesday from all days in 2018. Mutually Exclusive or Disjoint Sets cannot occur simultaneously. They have no intersection outcomes. For eg, A = day in Jan, B = day in Feb. A and B cannot occur simultaneously. In this, P(A1 U A2 U A3‚Ä¶) = P(A1) + P(A2) + P(A3)‚Ä¶ Also, P(A &amp; B) = 0. Collectively Exhaustive Events One of the event must occur The set of events covers the entire sample space For eg, A = Weekday; B = Weekend; C = January; D = Spring; Events A, B, C and D are collectively exhaustive (but not mutually exclusive ‚Äì a weekday can be in January or in Spring). Events A and B are collectively exhaustive and also mutually exclusive. Independent Events not dependent on each other. That is, occurrence of one does not affect occurrence of another event. Note: All mutually exclusive events are dependent but not all dependent events are mutually exclusive. 1.2 Addition Rule Addition rule of probability. \\[ P(A \\cup B ) = P(A) + P(B) -P(A \\cap B) \\] if mutually exclusive, then \\(P(A \\cap B) = 0\\). And is intersection, or is union. For eg, P(Jan or Wed) = P(Jan) + P(Wed) - P(Jan and Wed) = 31/365 + 52/365 - 5/365 = 78/365 1.3 Multiplication Rule For independent event, what happened in past event will have no effect on current event. For eg, P(HH) or P(at least 1H in 10 flips). \\[ P(HH) = 0.5 \\times 0.5 \\] P(at least 1H in 10 flips) = 1 - P(All T in 10 flips) \\[ 1 - (0.5)^{10} = 1023 \\div 1024 = 99.9% \\] 1.4 Marginal or Unconditional Probability: Simple probability like P(A) = 0.2, P(B) = 0.4 1.5 Joint Probability P(A &amp; B) both events to happen simultanieously 1.6 Conditional Probability When we have to find a probability under a given condition. Dependent Events A|B is ‚ÄòA happening after B‚Äô or ‚Äòconditional prob of A given that B has occurred‚Äô. B becomes the new sample space, because it‚Äôs A given B. Hence, \\[ P(A|B) = \\frac{P(A \\&amp; B)}{P(B)} \\] Independent Events if independent (does not affect each other), then \\[ P(A|B) = P(A) \\] Important outcome: When finding P(A &amp; B) we have to consider and analyse that whether A and B are dependent or not. Based on dependency, our P(A &amp; B) changes as follows: If dependent, the probability of A and B is: \\[ P(A \\&amp; B) = P(A) \\times P(B|A) = P(B) \\times P(A|B) \\] else \\[ P(A \\&amp; B) = P(A) \\times P(B) \\] because P(B|A) = P(B), occurrence of A has no effect on B. Probability of A or B \\[ P(A \\space or \\space B) = P(A) + P(B) - P (A \\&amp; B) \\] Add all the joint probability of colectively exhaustive and mutually exclusive events to get marginal probability of one event. eg, Consider industries and performance below. | Poor |Avg|Good|Marginal ‚Äî| small|0.02|0.07|0.01|0.1 medium|0.12|0.3|0.18|0.6 large|0.06|0.13|0.11|0.3 Marginal|0.2|0.5|0.3|1 1.7 Counting Events 1.7.1 Permutation Arrange \\(n\\) people in \\(k\\) seats. To count number of ways in which this can be done we use permutation. For eg, arrange 6 people in 3 seats, 6.5.4 = 6! / 3! = 120. \\[ _nP_k = \\frac{n!}{(n - k)!} = n(n-1)... (k times) \\] Used when order matters and pick once (without replacement). For eg, \\[ _{10}P_3 = 10.9.8 \\] 1.7.2 Combinations \\[ _nC_k = \\binom{n}{k} = \\frac{_nP_k}{k!} = \\frac{n!}{k!(n - k)!} = \\frac{n(n-1)...[k \\space times]}{k!} \\] We divide it by the number of ways in which k people can be arranged in k places, i.e, k! because ABCD and BCDA are same and we are counting this extra. Order doesn‚Äôt matter, 123 = 312. For eg, \\[ _{10}C_3 = \\frac{10.9.8}{3.2.1} \\] 1.8 Approach to solve a problem We can take following approaches to solve a probability problem use simple definition, \\[ P(e) = \\frac{events \\space possible}{sample \\space space} \\] Make a Contingency Table with possibilities. To find P(A or B), use P(A)+ P(B) - P(A and B) To find P(A and B), simply use (joint event)/total. To find P(A|B), P(A and B) / P(B) Make a Decision Tree, use when question has ‚Äúafter‚Äù. Find branches and outcomes Find effective value by multiplying with probabilities Roll back to find effective value at each branch. Use Venn Diagram when and/or is combined with not of a event. At least or at most, use \\[ P(at least/most) = 1 - P(e) \\] Example Find number of ways to arrange 1 - 10 digits in 3 places, Repetition allowed, order matters = 10.10.10 Repetition not allowed, order matters = Permutation = 10.9.8 Repetition allowed, order doesn‚Äôt matter = \\[ \\frac{10.10.10}{3.2.1} \\] Repetition not allowed, order doesn‚Äôt matter = Combination = \\[ \\frac{10.9.8}{3.2.1} \\] References: Khan Academy. AMPBA - ISB. "],
["basic-statistics.html", "2 Basic Statistics 2.1 Summarizing Quantitative Data 2.2 Spread and Variation of Data 2.3 Measure of Discrete Variables 2.4 Measure of Relations 2.5 Shape of Distribution", " 2 Basic Statistics Basics of statistics for Data Science. Central tendency measures tell us how values are grouped around the centre value. Variation tells us how disperse the data is or how much it is scattered Shape tells us the pattern of distribution of values. How the data is distributed in a frequency. Two type: Descriptive Inferential 2.1 Summarizing Quantitative Data Mean median and mode are three ways to summarize the data and to measure the central tendency of data. These are usually calculated for quantitative data. Mean Average or mean are one and the same thing. It is sum of all the data points divided by number of data points. \\[ ùúá = \\frac{ \\sum_{i=0}^n x_i }{ n } \\] It gets affected by the outliers . Median It is the middle number in an ordered list. If it‚Äôs even then it is sum of two middle number divided by 2, else it‚Äôs the middle one. if \\(n = even\\) then: \\((a+b)/2\\) where a and b are middle numbers. It is not affected by the extreme values. Mode It is the most common number or the most frequent number in the dataset. No mode, if the values in a given set all occur the same number of times, the data set has no mode because no number is any more common than any other. Several mode if more than one number is repeated same number of times. It is not affected by extreme values. Geometric Mean It is used to measure rate of change of variable over time. For eg, rate of return on investment over years. \\[ X_G = (X_1 \\ x \\ X_2 \\ x\\ ... \\ x\\ X_n)^{(1/n)} \\] GM rate of return, here \\(R_i\\) is rate of return in time period i \\[ R_G = [ (1 + R_1) \\ x \\ (1 + R_2) \\ x ... x \\ (1 + R_n) ]^{(1/n)} - 1 \\] 2.1.1 Effect of Outliers Removing a big outlier decreases the mean more but less change on median and may be median doesn‚Äôt change. 2.2 Spread and Variation of Data The spread of data can be measured by its range, interquartile range (IQR), variance, standard deviation. and coefficient of Variation. Range It is difference between largest and smallest value in data. It is not dependent on distribution of data and is sensitive to outliers. Interquartile Range Quartiles divide the ordered data in to 4 segments. Each have equal number of values. Median (Q2) divides the dataset into two different parts. IQR is the difference in the middle of the first half (Q1) and the middle of second half (Q3). We then find the median of these two different parts and then find the difference. If we have even number of data points in dataset then we include the first middle number in first set and the second middle number in second set. IQR is useful to find out how much the data is varying. Range can be misleading when we have outliers. But in this case interquartile range can give us much better measure of spread of data. \\(IQR\\) = md 2nd quartile - md 1st quartile = \\(Q_3 - Q_2\\) Quartile Measures Q1, is the value for which 25% of the observations are smaller and 75% are larger Q2 is the same as the median (50% of the observations are smaller and 50% are larger) Only 25% of the observations are greater than the Q3 A box plot shows Min, Q1, Q2, Q3 and Max values of data. Measure of spread Measure of spread can be found by calculating range variance and standard deviation. Sample is part of data while population is entire dataset. We can estimate population by using these measures from the samples. Variance Squared deviation of values from the mean \\[ ùúé^2 = \\frac{ \\sum(x_i - ùúá)^2 }{ n } = \\frac{ \\sum(x_i)^2 }{ n } - ùúá^2 \\] Standard Deviation Variation about the mean. It has same unit as the original data. \\[ ùúé = \\sqrt{Variance} = \\sqrt{ùúé^2} = \\sqrt \\frac{ \\sum_{1}^n(x_i - ùúá)^2 }{ n }\\] For example: A = [-10, 0, 10, 20, 30] B = [8, 9, 10, 11, 12] Mean of two data set is same (10) but range varies. So we calculate variance to show difference between datasets. \\[ ùúé^2(A) = 200 \\] \\[ ùúé^2(B) = 2 \\] \\[ ùúé(A) = \\sqrt{200} = 10\\sqrt{2} \\] \\[ ùúé(B) = \\sqrt{2} \\] Result Hence, points in A are 10 times more deviated. Note: The more the spread, the greater is range, variance and SD The more data is contracted, the smaller these measures are. Standard Deviations tells us how far we are from the mean. If all values are same (no variation) then all these are zero. None of these measures can ever be negative. Adding a number to all values, \\(x_i\\), makes no difference to variance. Multiplying a number, k, to all values, \\(x_i\\), makes variance \\(ùúé^2 \\times k\\) Better measures What defines dataset better, mean or median? Mean can give us the standard deviation, median can you give us IQR. Mean is better for symmetric dataset, median is better for skewed dataset which has outliers. Median is better for salaries and home prices as it has outliers. 2.3 Measure of Discrete Variables Discrete Variable has a finite outcome. It has a fixed values. The events are mutually exclusive. 2.3.1 Expected Value This is measure of center. Also called weighted average. \\[ ùúá = E(X) = \\sum_{i=1}^N X_i P(X=X_i) \\] This is sum of each (value x probability) 2.3.2 Expected Variance Difference from mean, squared times probability, then sum: \\[ ùúé^2 = \\sum_{i=1}^N [X_i - E(X)]^2 P(X=X_i) \\] 2.3.3 Expected Standard Deviation Difference from mean, squared times probability, then sum: \\[ ùúé = \\sqrt{ùúé^2} = \\sqrt{ \\sum_{i=1}^N [X_i - E(X)]^2 P(X=X_i) } \\] 2.4 Measure of Relations 2.4.1 Coefficient of Variation Coefficient is the multiplicative factor. That is how many times a variable is of another variable. Here we compare variation and mean. So it variation relative to mean. Always in percentage, and can compare data with different units. \\[ CV = \\frac{ S }{ X } \\times 100\\% \\] For example, Stock A = $50, SD = $5 CV(A) = (5/50)*100% = 10% Stock B = $100, SD = $5 CV(B) = (5/100)*100% = 5% Hence, both stocks have same SD, but stock B is less variable relative to its price. 2.4.2 Covariance Measure of strength of linear relationship between two discrete random variables X and Y. \\[ ùúé_{XY} = \\sum_{i=1}^N [X_i - E(X)][Y_i - E(Y)] P(X=X_i,Y=Y_i) \\] where, \\(P(X=X_i,Y=Y_i)\\) is probability of occurrence of the i outcome of X and the i outcome of Y. 2.4.3 Coefficient of Correlation The relative strength of linear relation between two variables is called correlation. It is between -1 and 1. \\[ r = \\frac{cov(X,Y)}{S_XS_Y} \\] This is covariance relative to standard deviation. In above, \\[ cov(X,Y) = \\frac{\\sum_{i=1}^N [X_i - \\overline{X}][Y_i - \\overline{Y}]}{n-1} \\] \\[ S_X = \\sqrt{ \\frac{\\sum_{i=1}^N [X_i - \\overline{X}]^2}{n-1} } \\] \\[ S_Y = \\sqrt{ \\frac{\\sum_{i=1}^N [Y_i - \\overline{Y}]^2}{n-1} } \\] Cor = 0, means no linear relation, but may has non-linear relation. In investment portfolio, expected return and standard deviation of two funds together can be calculated. 2.5 Shape of Distribution This tells us how data is distributed. It can be measure by: Skewness: Extent to which data values are not symmetrical Kurtosis: Affects the peakedness of the curve of the distribution. It tell the sharpness of rise of curve. Bell shaped is Mesokurtic (Kurtosis = 0). 2.5.1 Skewness Data can be symmetric, left or right skewed. If it is symmetric then we can work on half of the sample of data. Left: Mean &lt; Median Symmetric: Mean = Median Right: Median &lt; Mean 2.5.2 Kurtosis How sharply the curve rises. Eg: High rise, Kurtosis &gt; 0, Leptokurtic. Bell shaped, Kurtosis = 0, Mesokurtic. Low rise, Kurtosis &lt; 0, Platykurtic. Image of shape and quartiles and box plot "],
["advance-statistics.html", "3 Advance Statistics 3.1 Frequency Distribution 3.2 Probability Distribution Functions (PDF) 3.3 Sampling Distributions 3.4 Estimators and point estimates:", " 3 Advance Statistics This note covers distributions, Confidence Intervals. 3.1 Frequency Distribution We can break data into different classes, then find frequency of data in each class. It is what we see in histogram. Each class has a mid point, a frequency. We can find Relative Frequency which is frequency divided by total frequency (number of data points). So this give us percentage of data point is a particular class interval. For eg, class A (10 but less than 20) has frequency 3, total observations 20, so relative freq 0.15, hence, 15% data points are in class A. We can also find cumulative frequency and relative cumulative frequency or Cumulative Percentage. It can help us find probability that a data is under or less than that class interval. Use of Frequency Distribution Raw to useful form Visually see the data distribution See interval in which data is contracted or clustered. Tips: Play with class interval to see different picture of data. In large dataset, boundaries don‚Äôt make much difference. When comparing different groups with sample, use relative frequency or percentage distribution. 3.2 Probability Distribution Functions (PDF) A frequency distribution can be a probability distribution if the area under the curve is equal to 1. The f(x) of Prob Dist gives us probability of occurrence of x in the given distribution. Random Variables RV, \\(X\\), are variables that can have any random value \\(x\\). For eg. rolling a die. \\(P(X=x)\\) is probability that X has outcome x. \\(F(x)\\) = PDF \\(E(x)\\) is expected value = weighted average RV can be of two types: Discreet, fixed values, finite outcome, mutually exclusive events. Continuous, any value in range, data points are approx values. Discreet variables PDF: Bernoulli Distribution Binomial Distribution Poisson Distribution 3.2.1 Bernoulli Distribution The event has two outcomes, so probabilities are \\(p\\) and \\(p-1\\). 3.2.2 Binomial Probability Distribution When we have to make \\(k\\) success in \\(n\\) attempts, then we can following formula: \\[ P = _nC_k . œÄ^k . (1-œÄ)^{(n-k)} \\] Here œÄ is probability to get success. eg, throwing a basket ball. Now if we move k from 0 to n, we get different values of P. This can make a distribution. We call this binomial distribution. 3.2.3 Poisson Probability Distribution Continuous RV can have PDF: Uniform or Normal. 3.2.4 Uniform Distribution Here, probability of each outcome is equal. Hence, funcition is 1/range: \\[ f(X) = \\frac{1}{(b - a)} \\] where a is min and b is max. Measures: \\[ ùúá = \\frac{a + b}{2} \\] \\[ ùúé = \\sqrt{ \\frac{(b - a)^2}{ 12 } } \\] Probability that 3 &lt;= x &lt;= 5, is area under the line between 3 and 5. As it is a rectangle, the area can be (base)(height). 3.2.5 Normal Distribution It is: Bell Shaped Symmetrical Mean, Median and Mode are equal The random variable has theoretically infinite range. It is defined by: \\[ f(x) = \\frac{1}{\\sqrt{2\\pi}ùúé}e^{-\\frac{1}{2}(\\frac{X - ùúá}{ùúé})^2} \\] e = 2.71828 \\(\\pi\\) = 3.14159 Mean moves distribution left/right, sd increases/decreases spread. P(-‚àû &lt; X &lt; Œº) = 0.5 and P(Œº &lt; X &lt; ‚àû) = 0.5 3.2.6 Standardized Normal or Z score Any normal distribution can be transformed to standard normal distribution \\((Z)\\) using mean and sd. Need to transform X units to Z units. It has mean of 0 and SD of 1. \\[ z = \\frac{X - ùúá}{ùúé} \\] Also, known as Z distribution. For eg, if mean=100, sd=50 then Z for X=200, (200-100)/50 = 2 This says that X = 200 is two standard deviations (2 increments of 50 units) above the mean of 100. So for X and Z distribution the shape remains the same, only scale changes. Note: We measure a few scores compared to the mean. Like performance in class. How far are we from the mean in terms of SD. are we 1sd far from mean or so on. One such score is Z score. 3.2.6.1 Calculating Probabilities The probability is area under the curve. So P(a &lt;= X &lt;= b) is area under curve between x=a and x=b. Note: P(a &lt;= X &lt;= b) = P(a &lt; X &lt; b), as P(a) or P(any point) = 0. The total area under the probability curve is 1 and curve is symmetric so half above mean and half below. To calculate probability of any RV in a range we need to find area under the curve in that range. So easily find the answers we convert every normal distribution to standard Z distribution. Then from Z Table we look up values for that value of Z. The score at a value gives area from -\\(\\infty\\) till that value. Z Table is cumulative probability of standardized normal. the sum of table headers are Z-scores, or how many standard deviation. the table values are probabilities. For eg, Z(2) = 0.9772, this is P(Z &lt; 2, till -infinity) To find a probability for normal X distribution, convert X to Z the find Z value for it. Most used Z-scores: 90% = 1.645 95% = 1.96 99% = 2.576 Excel formula 95% CI =-NORM.S.INV((1-0.95)/2) 3.2.6.2 Empirical Rules 1œÉ covers 68.26% of X 2œÉ covers 95.44% of X 3œÉ covers 99.7% of X Above, everything was calculated for a population. Now we will talk about the samples. 3.3 Sampling Distributions Population is the large group of data that we want to study. We pick a sample of people and try to compare how they perform compared to the population. We collect different sample from the population for example 50 different records from a population. There will be variation in each of these different samples. So each of these sample gives us slight variation from each other. So sample A is different from sample B and so on. When we collect different samples and find their mean or sd, then this set of information makes sample distribution. 3.3.1 Developing a Sampling Distributions If I choose every possible sample of size ‚Äún‚Äù from a population then I get ‚Äúsampling distribution‚Äù. Now if we collect mean of each of these possible sample then we get ‚ÄúSampling distribution of Sample Means‚Äù. When we make probability distribution of a population that is unbiased, there are equally likely chance to pick a number. So it is a uniform distribution. Next, we take all possible sample and find mean of each. The prob distribution of the mean of sample will be a normal distribution. So we find \\(ùúá_{\\overline{X}}\\) and \\(ùúé_{\\overline{X}}\\) and n, here n is number of items in each sample. x_ is a random variable and not ùúá, mu is fixed. If we find all different mean of samples, xi then 90% CI of each xi will have ùúá. Standard Error of the Mean It is variability in the mean from sample to sample of same size. Standard deviation of mean of means. For eg, if we take random samples of size n from a population, we get mean for each sample, \\(X_i\\) . Different samples have different mean, not all are equal to population mean. S(A) give mean X(A) and so on. Now all these means \\(X_i\\) = [X(A), X(B), X(C)..] form a distribution. The standard deviation of this distribution gives us Standard Error. \\[ ùúé_{\\overline{X}} = \\frac{ùúé}{\\sqrt{n}} \\] If population is very small: \\[ ùúé_{\\overline{X}} = \\frac{ùúé}{\\sqrt{n}} \\times \\sqrt{\\frac{N-n}{N-1}} \\] Note: The standard error of the mean decreases as the sample size increases. If population is more and samples is less than 1% then we can ignore (N-n)/(N-1) as this equals to 1. Margin of Error if we know the width, +-100, and we know sd for population and we know confidence interval so we can find the sample size that can help us achieve this. DMOE = Margin Of Error sigma = 4608, dmoe=100, z=1.96 This is sample error: \\[ \\] 1.96*(4608/n**(0.5)) = 100 n = pec half, sample 4 times, because of square. inc confidance, z inc, sample inc. Sampling Error There is an error associated with mean of a sample. Because it is not exactly same as population mean mu. We can find a sample size to get desired margin of error (e) with (1 - Œ±) level of confidence. \\[ e = Z_{Œ±/2}\\frac{ùúé}{\\sqrt{n}} \\] \\[ n = \\frac{Z_{Œ±/2}^2 ùúé^2}{e^2} \\] For eg, what can be the sample size if we want error of \\(\\pm\\) 5 with 95% confidence interval when œÉ=20. 3.3.2 Central Limit Theorem It says that the distribution of mean of samples is mostly normal distribution. It is not dependent on shape of original population. Irrespective of what population looks like, sample means always follow normal distribution \\[ \\bar{X} =N (\\mu ,\\frac{\\sigma}{\\sqrt{n}}) \\] Here, N is normal distribution. For any population, if we take samples and plot mean of samples, it always follows normal distribution. the mean is almost equal to pop mean sd is $ / $ n is sample size $ _{x} $ is small then more reliable ùúá. if sigma xi is more that means more spread out sample means, hence less reliable x_ 3.3.3 Point and Interval Estimates Point is a single number, X, in population. We can find confidence interval for that number. Interval estimate tells us more information about the population than a point estimate tells us. So for eg, 5kg rice packet has SD of 50gm in population. So 3SD would contain 99% of rice packets. Now if we take samples and find mean of samples it would give us a distribution. We can take one Point near 5kg, then we can find the confidence interval in which this mean will lie with a percentage confidence. Like, 95% confidence that x=49.94 is mean and lies between found interval. Point estimate \\(\\pm\\) (Critical Value)(Standard Error) Point is the sample statistic estimating population parameter of interest. Critical Value is z table value, based on confidence interval Standard error is SD of point estimate. 100% is close to infinity, 100% sure that mean will lie between infinities. 95% is also large interval. 0% confidence would be a very small interval. 3.3.4 Confidence Intervals and Levels From a sample, we can infer the population parameter by finding its interval and a confidence. For eg, order size. From a sample, we can find CI and say, ‚ÄúWe are 95% confident that the order size of population would be between 990 to 1100‚Äù. How to find CI Let alpha, Œ± = probability we are not interested in, the point that lie outside 95%. =&gt; (1 - Œ±) = 0.95 =&gt; Œ± = 0.05 =&gt; Œ±/2 = 0.025 Now, we are interested in points where the probability left out is 0.025, the lower tail, that is -infinity to lower point. In Z table, the value 0.025 can be found for Z score 1.9 and .06. So 1.96 is Z score. Hence, \\(Z_{Œ±/2}=\\pm1.96\\), also called critical value. This tells us that on a standard curve, sd=1 and mean=0, the 95% confidence interval is +- 1.96. If we go 1.96 sd to left and right of mu, we cover 95% data. To find actual values of actual normal curve, we use the formula Point estimate \\(\\pm\\) (Critical Value)(Standard Error). CI, when œÉ is known: \\[ \\overline{X} \\pm Z_{Œ±/2} \\frac{ùúé}{\\sqrt{n}} \\] Increasing Precision If we increase the size of sample, n then CI range decreases. So for same confidence we are finding less range interval, this increases precision. Use Cases: For cost benefit analysis, for eg, minimum orders we can expect. If that order size makes our new business line profitable, go ahead. Find CI, CI tells us, that we can expect order size within an interval, So for 90% CI, it says we are 90% confident that the ùúá lies in interval. So definately we can expect orders more than lower limit. We do have 5% risk involved. We can further reduce it by finding 95% CI, but that reduces our precision as it increases the interval width. Smaller the range of CI, more precise we are. Our aim should be to increase the accuracy with precision. Increase % but also decrease range. eg, 90% conf =&gt; 90 &lt; x &lt; 110, to 98 &lt; x &lt; 102, this should be our approach. 3.3.5 Confidence Interval Sigma Unknown What if we don‚Äôt know population standard deviation, sigma? Then we use t-Distribution. it was stated by William Gossett - ‚ÄòThe probable error of mean‚Äô Degree of Freedom, df = (nCol - 1) X (nRow - 1), from a contingency table. When we do not know sigma then we estimate it using proxy. We can substitute the sample standard deviation, S. This introduces extra uncertainty, since S is variable from sample to sample.- So we increase the interval to cover this uncertainity So we use the t distribution instead of the normal distribution. why we use (n-1) in denominator? To add uncertainity. use one proxy, loose one degree of freedom. (n-k) loose k degree of freedom. We add uncertainity in the confidence interval to keep it safe. Student‚Äôs t Distribution It is a family of distributions. Its value depends on degrees of freedom (d.f) = n - 1. Degree of freedom is a number of observation that can take any value after we have mean calculated. t -&gt; Z as n increases. \\[ \\overline{X} \\pm t_{Œ±/2} \\frac{S}{\\sqrt{n}} \\] where \\(t_{Œ±/2}\\) is the critical value of the t distribution with \\(n-1\\) degrees of freedom and an area of Œ±/2 in each tail df = 1 , then broder and shorter compared to normal distribution. as we inc df, it becomes taller and thinner. it gets close to normal distribution. as n approcahes infinity, t-distn becomes normal distn. Conclusion: So, if you don‚Äôt know sigma, use t, for any degree of freedom and any conf interval. If you know sigma, never use t-distribution. Use, \\(S_\\bar{x}\\) to find CI. Same as we do using sigma x bar. \\[ \\sigma = \\sqrt{ \\frac{\\sum_1^n (x_i - \\mu)^2}{N} } , \\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{N}} \\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space S = \\sqrt{ \\frac{\\sum_1^n (x_i - \\bar{x})^2}{(n-1)} } , S_\\bar{x} = \\frac{S}{\\sqrt{n}} \\] Population Sample (Proxy) $ $ = Avg of population, mean $ {X} $ = Avg of sample N = values in population n = values in sample \\(\\sigma\\) = Std Dev of population S = Std Dev of sample $ _{X} $ = St Error of population, Sd of sample means $ S_{X} $ = St Error of sample \\[ CI = \\mu \\pm Z_{\\alpha/2}*\\sigma_\\bar{x} \\] Find CI of Population Mean: Find avg, x-bar and sd of sample, S find S-X-bar, S by root N find t value of Confidence level, using, =T.INV.2T(1-alpha,n-1) find upper and lower limit, CI by, S +- (t * S-X_Bar) Use this CI to tell that mu of population lies between this interval. Find ‚Äòn‚Äô samples required for desiered Confidence and Interval and Average expected For, e.g., if we need, 99% accuracy with a margin of 0.01mg in tablets then we can find sample size required. find t-value based on conf% 3.4 Estimators and point estimates: point estimate is a point we estimate from a sample estimator is conf interval point is exactly in the middle of conf intvl they are both statistic e.g. x bar for mu and s^2 for sigma sq. 3.4.1 Population Proportion Just like mu we‚Äôll build CI for pop proportion ùúã. Sample proportion ùëù. Hit rate is proportion of people buying sarres Based on sample proportion, I can draw population proportion. to estimate population proportion form sample proportion we need to find probability distribution. The we can use all CI adn CLT Proportion is binomina, success/faliure, interested/non interested. It is state of mind. prop depends on sample, bw 0 and 1. binomial distn, success or faliure, BD, will buy or not? (ùúã) is mean of appropriately constructed random variable. is sample mean of random variable. E(x) = n pi var(x) = n pi (1 - pi) Q ~ B{n pi, npi(1-pi)} if x = {0,1} binomial, then p= Q/n = exi/n, all 0 sum to 0 mathematically identical to x bar. ci = p +- Za/2, sigma p cap, cap is proxy. 3.4.2 P Value (needs elaboration) It is same as probability but also take equally likely outcome and rare outcome. So p(two heads, HH) = 0.25. p value of HH is P(HH) + P(TT) + 0 for rare. "],
["data-handling-etl-eda-and-preprocessing.html", "4 Data Handling - ETL, EDA, and Preprocessing 4.1 Analyzing Dataset - Data Understanding 4.2 Data Preprocessing 4.3 Data Wrangling", " 4 Data Handling - ETL, EDA, and Preprocessing Data handling includes reading, understanding, cleaning, transforming, pre-processing and loading data. How to do EDA? When doing data analysis EDA is step 1. EDA is done to understand your data as much as possible. It make yor familiar with data distribution, data variables and relation of variables. This guide is like a pseudo-code for doing EDA. So let‚Äôs get started with.. What is Exploratory Data Analysis? It is the very first step in analysis of the dataset. It is the step to prepare data before applying any model. But before EDA, one should spend time in really understanding the problem, as in why one needs to do EDA. Once you are clear with the problem statement you can relate the data to solution while doing EDA. Find the key variable that is required to solve the problem. For e.g., in titanic data set it is ‚Äòsurvived‚Äô. In house price data set it is ‚ÄòSales Price‚Äô. Then begin with univariate analysis on thsi data variable. Find relation of key variable with other variables, this is bivariate analysis. Once, this is done, try to handle outliers and missing values. It can include many steps as follow: 4.1 Analyzing Dataset - Data Understanding Look at the metadata of the variables. What variables are available. what information do they hold. what data type they have. brief stats around those data variables. We can use following commands: df.head() df.info() df.descibe() Analyze key component In this section, use statistics and visualization to analyze one (or more) of the variable from the data set. For eg, sales price. Use df['col'].describe() to see stats output. Find distribution of the key component by plotting a distplot() or histogram. Here, we can see if it is normally distributed or skewed. Analyze relation of Key component with other Variables We now analyze key component variable with respect to other numerical variables availabe in the dataset. To do this we can make scatterplot() between two different variables. This will show us the relation between them. Linear relation or not. Positive or negative.. etc. To analyze categorical variables use boxplot(). Finding Correlation Use .corr() to find correlation between all variables. THen use heatmap() to plot the correlations. Find which varables have strong correlation. Find fields of interest, then make a zoomed heatmap to further analyze variables or remove variables which are not of use. Once we ahve found variables that have good relation with key variable we can create a pairplot() of the columns of interest to compare how they relate with each other. Now that we have found data variables that make sense and how they are related, we can think about further cleaning the data, like handling missing values and outliers. 4.2 Data Preprocessing Steps reuquired to clean and build data ready for machine learning. Sample dataset: sample dataset 4.2.1 Missing Data We can use isnull() to find missing values. Find total number of missing values and % of missing values for each column. Consider how much impact missing values will have on analysis. If the columns having missing values are not the ones of our interest, drop the columns. If total missing values are very small compred to number of records, simply you may drop one or few rows. you can remove if you don‚Äôt have crucial info and have lot of data replace by mean of column from sklearn.preprocessing import Imputer imputer = Imputer(missing_values = &#39;NaN&#39;, strategy = &#39;mean&#39;, axis=0) # Imputer is class, imputer is obj imputer = imputer.fit(df[&#39;age&#39;,&#39;salary&#39;]) # we pass matrix from df, it is two columsn with numbers. # we get fitted object imputer, fitted to our matrix df[&#39;age&#39;,&#39;salary&#39;] = imputer.transform(df[&#39;age&#39;,&#39;salary&#39;]) 4.2.2 Categorical Variables ML models can only take numbers in mathematical eqns. So encode to numbers. eg, country, boolean (yes/no). For comparable and booleans use LabelEncoder, it gives numbers for categories. from sklearn.preprocessing import LabelEncoder labelEncoder_y = LabelEncoder() df[&#39;y&#39;] = labelEncoder_y.fit_transform(df[&#39;y&#39;]) # fits transforms and returns encoded values, a vector y For non-comparable categories, like country, use OneHotEncoder. It makes dummy_variables with each country having its own column. Now numbers are comparable so country with 2 will be treated greater than 1, but this doesn;t make sense so we will use dummy_variables. So we have new columns equal to number of categories. so we will have column for each country and it will ahve 0 or 1 depending on its presence. for this we use: from sklearn.preprocessing import OneHotEncoder oneHotEncoder = OneHotEncoder(categorical_features = [0]) # index of categorical variable df = oneHotEncoder.fit_transform(X).to_array() 4.2.3 Train Test Validation Split We need to divide the data in to train and test so that our model does not learn the correlation by heart and can fit new data which is slightly different from train. We need to be efficient in making the split so we use library. cross_validation library is used for this. from sklearn.cross_validation import train_test_split # X are matrix of observations. y is dependent variable. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) # random_state is used to get same split everytime. 4.2.4 Feature Scaling Our observations can be at different scales, eg, age ranges 20-60 and salary ranges 40k-90k. Our ML model depends mostly on euclidean dist, \\[ P_1 (x_1, y_1) and P_2 (x_2, y_2) \\] \\[ dist = \\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2} \\] ML model computes dist between two rows to use a model. so difference of square can be dominated by var having much range, salary in this case. Hence we need to scale variables. Other ML model which do not depend on eucledian dist like decision trees that will converge fast. ML model will run faster. Scaling: \\[x_{standard} = \\frac{x-\\mu}{\\sigma} \\] mean and std dev \\[ x_{norm} = \\frac{x-min(x)}{max(x)-min(x)} \\] This gives same range and same scale. from sklearn.preprocessing import StandardScaler sc_X = StandardScaler() X_train = sc_X.fit_transform(X_train) X_test = sc_X.transform(X_test) Scaling dummy variables depends on the context, if they make significant effect on model then scale them. We loose context when we scale them but may bring better accuracy. If any data is not normally distributed then we can take log of that data considering it doesn‚Äôt affect much. (this needs improvement in understanding). ToDo: May include PCA LDA here, in future 4.3 Data Wrangling Pandas is a package in Python that can be used for data manipulation. 4.3.1 What is Data Manipulation? Data manipulations can be organized around six key verbs: arrange: order dataframe by index or variable or sort the data select: choose a specific variable or set of variables or select columns in data filter: subset a dataframe according to condition(s) in a variable(s) or select rows in data mutate: transform dataframe by adding new variables or add a calculated column group_by: create a grouped dataframe summarize: reduce variable to summary variable (e.g. mean) Here, variable is a column in data set. We‚Äôll cover how to perform above operations on a dataset using Pandas. 4.3.1.1 Filter We can filter data to get a set of rows from complete dataset. It is similar to WHERE clause in SQL. 4.3.2 Doing same stuff using R R is also an excellent programming language for data manipulation. dplyr is a package in R that can be used to perform above operations. References: An excellent article by Ben, The 5 verbs of dplyr, can provide you more details on this. Another article that compares R and Python can be found here. Comparison of Pandas with SQL Pandas docs excellent details with examples. "],
["logistic-regression.html", "5 Logistic Regression", " 5 Logistic Regression Logistic Regression is mainly used when we have to work on probability. It outputs values between 0 and 1. So 0.5 is cutoff point and anything below 0.5 goes to class 0 and rest to class 1. Logistic is also called Sigmod. Shown in function below. \\[ \\phi (z) = \\frac{1}{1 + e^{-z}} \\] this always outputs b/w 0 and 1. 5.0.1 Difference between logistic and linear equations linear model: $ y = b_0 + b_1x $ where as logistic is shown in equation below (also same as \\(\\phi (z)\\) eq above) : \\[ p = \\frac{1}{1 + e^{-(b_0 + b_1 x)}} \\] 5.0.2 Model Evaluation Use confusion matrix to evaluate classification model. Classifiaction model is when we classify our predisctions against test data. Confusion matrix puts predictions and tests together to compare. n=165 Predicted=NO Predicted=YES Actual: NO TN = 50 FP = 10 60 Actual: YES FN = 5 TP = 100 105 55 110 False Positive FP is Type 1 error FN is Type 2 error 5.0.3 Accuracy: Correct: (TP + TN) / total = 150/165 = 0.91 Our model is 91% correct, Wrong: (FP + FN)/100 = 15/165 = 0.09 Our mode is 9% wrong. "],
["blockchain.html", "6 Blockchain 6.1 Why Blockchain? 6.2 What is Blockchain: 6.3 Blockchain implementations 6.4 Blockchain in Healthcare 6.5 Blockchain in Banking Industry", " 6 Blockchain Blockchain is a digital, immutable, distributed ledger that chronologically records transactions in near real time. The prerequisite for each subsequent transaction to be added to the ledger is the respective consensus of the network participants (called nodes), thereby creating a continuous mechanism of control regarding manipulation, errors, and data quality - Deloitte Do not remove this line (it will not be displayed) {:toc} 6.1 Why Blockchain? We are now interconnected, global, ecosystem with huge data, cheap storage and enormous computing power, also there is erosion of trust (in pharmaceuticals, in money value,), desire for freedom and privacy, no middleman, low faith in government and distrust in organizations, we need decentralized, open, trustable, secure, chain of data to hold all our information in transactional format. Hence, blockchain. 6.2 What is Blockchain: it is data structure that holds transactional records with ensuring security, transparency and decentralization. it is immutable, it is secure with data hashed and cryptographically sealed, some have added level of security with multiple abstraction. it is distributed ledger that is verifiable. each block has hash, cryptography - it is one way hash function, it can‚Äôt be decrypted. data and timestamp, hash of previous block, together they make chain. A Blockchain is a digital, immutable, distributed ledger that chronologically records transactions in near real time. The prerequisite for each subsequent transaction to be added to the ledger is the respective consensus of the network participants (called nodes), thereby creating a continuous mechanism of control regarding manipulation, errors, and data quality - Deloitte It uses old technologies all together to from a chain. It is a transaction processing system (TPS) 6.2.1 Blockchain Flow: Someone requests a transaction. transaction is broadcasted to P2P nw consisting of nodes, waiting pool. Miner picks up, P2P nw validates the transaction and user‚Äôs status using consensus (known algorithms). Once verified, the transaction is combined with other transactions to create a new block of data in the ledger. New block is added to existing blockchain in a way that is permanent and unalterable, This completes the transaction. 6.2.2 Consensus Mechanism: agreement to verify a block to be added to blockchain. agreement on a single data value among distributed process like cryptocurrencies. Methods: Proof of Work (PoW) - miners compete against each other to complete transactions on the network and get rewarded. This is used by BTC and does not require all members to conclude in order for a consensus to be reached. Proof of Stake (PoS) - a person can mine or validate block transactions according to how many coins he or she holds. Proof of Authority (PoA) - consensus mechanism based on identity as a stake. It is fast. RAFT - easy BFT is a way for a distributed network to reach the consensus set for the Blockchain even if some nodes are malicious. Proof of Importance (PoI) - sed to determine which users are eligible to perform the calculations necessary to add a new block of data to a blockchain and receive the associated payment. 6.2.3 Types of Blockchain: Public open any one can participate and see blocks any one can be part of consensus. Permissioned/close loop right to add a block is only to a few selected people. Private (Business Blockchain) write access is with a company. not public but confined to business. not everyone can participate. B2b, centralized. the digital assets are real state, crops, commodities, etc., so represents economic values. uses known algorithms, guaranteed commit, Power vs Distribution Power/Distribution Local Setup Distributed over Network Decentralized Power Bank Lockers - Bitcoin - Public Blockchain - Global chains Centralized Power Corportions - e.g. SBI - Permissioned Blockchains - Corporate Blockchains - Consortiums Blockchain Trilemma Scalability, security and Decentralization. You can achieve 2 of above to maximum extent. It is slow to process transaction per second It is secure but no guaranty of regulations. 6.3 Blockchain implementations 6.3.1 Bitcoin BTC It is digital currency, transactions are stored in ledger called blockchain. Bitcoin mining is adding transaction records to public ledger. Why 21m BTC? - so that when demand is high the value can go up. they are to be mined by 2140. 6.3.2 Hyperledger Fabric open source project by linux, IBM, SAP to use blockchain technology for distributed ledger. It is modular blockchain framework with components that can be used to make blockchain based products and solutions and is aimed for use in private enterprises. It is aimed for enterprises and not for cryptocurrency 6.3.3 Ethereum open source, programmable blockchain runs Smart Contracts lines of code in blockchain, line of code is agreement/rules between people. open and immutable due to blockchain executes when conditions are met essentially computer codes stored in a Blockchain to process pre-defined business steps and execute a commercial/ legally enforceable transaction without involvement of an intermediary. users can make agreement/transactions to buy/sell/trade without middleman. currency is Ether. 6.3.4 Agritech Blockchain Blockchain can be used to enhance: Food Safety Traceability Transaction Costs Opening New Markets Logistics 6.4 Blockchain in Healthcare it includes r&amp;d, pharma, chemists, doctors, they are part of eco system. drug verifications, ownerships, contracts, product verifications, 6.5 Blockchain in Banking Industry What is Banking Industry: Banking Industry provides the liquidity needed for individuals and businesses to invest in the future. Bank loans and credit helps individuals afford a college or buy a house without having all the money. Companies use loans to start functioning immediately to build for future demand and expansion. Banking all together helps a country grow and maintain the GDP of the country. It is related with the economy of a country. Hence, there are many participants that are directly linked with the banking industry. Regulation authorities are also part of banking industry that keep an eye on bank and help it survive in situation of a crisis. 6.5.1 Blockchain features and its use in Banking Industry Blockchain has may factors that make it a solid secure, immutable, distributed and trusted technology. These can make it a go to technology for banking industry if we tackle the challenges and issues that we face currently in the domain. Some of the factors that will help are: Immutable blocks: Blocks in blockchain can only be store once. They cannot be modified and makes the content like intellectual properties, agreements secure and tamper proof. Trust: Blockchain uses ‚Äòsmart contracts‚Äô which builds transparent ledgers hence make it easier for different parties to collaborate and come to agreements. Blockchain allows automation of business processes through the creation and execution of smart contracts. Privacy: Privacy technologies can use closed private blockchain enabled by blockchain technology for selective sharing of data between businesses. Performance: The networks are engineered to sustain a high number of transactions while supporting interoperability between the different chains, creating an interconnected web of blockchain. Distributed: Blockchain technology architecture eliminates single points of failure as it is distributed and hence reduces the need to place data in the hands of middle mans. Transparency: Blockchain standardizes shared processes hence it creates a one shared source of truth which is open for all network participants. Asset Tokenization: Blockchain technology provides technological base layer that enables the easy tokenization of all types of assets. Financial assets can be tokenized using blockchain hence it is a convenient solution. Buying and selling securities and other assets requires a complex and coordinated effort between banks. 6.5.2 How blockchain can help Banking Industry grow and achieve pre-eminence? There are many areas in which Banking Industry can grow: Fraud Reduction‚Äì Stock trading and money transfer are prone to frauds. This can be prevented using blockchain. The permissioned, cryptographic and decentralized nature of blockchain makes it highly secure against frauds. Centralized databases are easy target for attackers but using blockchain this data can be distributed and hence it will become more safe. Decentralized Trust - blockchain works by verifying and tracking transactions. It eliminates third party organizations for processing transactions. This will help bank transact without depending on intermediaries and third-party validation of transactions. This reduces or even eliminates counter-party risk. Enhanced Security - When the data is recorded in a block, its immutable, hence secure. As it is decentralized, it is difficult to shut down or hack. Also it can be viewed by anyone in the system, hence transparency. Exchange of transactional value is governed by strict cryptographic rules in blockchain hence the use of unique digital signatures reduces the risk of fraud. Also as it is decentralized it does not have a central point of failure. Cost Reduction - Banks are able to decrease transaction fees significantly by eliminating third party intermediaries. Also overhead costs for exchanging assets is reduced as the distributed ledger approach is used to form a system that decentralizes trust. Cross-border payments are made easier as middle man is eliminated. Trading and settlement to become quicker, more reliable and less expensive. Increased Efficiency - Chances of errors and duplication are reduced in blockchain. It is ideal for refurbishing a range of digital processes. In blockchain the data is stored in blocks using an immutable and tamper-proof format hence it improves the mobility of data and decreases the time taken for KYC efforts. The removal of third party reduces the settlement time to few seconds and hence the transaction time to minutes. Blockchain data is complete, correct, and trustworthy. Payments - Blockchain is secure and eliminates third party, hence payments are more secure and faster, it can also be done with lower cost as it does not involve any third part or middle man. Also cross-border payments are easier using blockchain. Thus blockchain improves the payment processing system. Smart Contracts - Two parties can make contracts very easily using smart contracts in which agreement can be made using the computer code. This executes only when the two parties enter their keys. Hence it only executes according to the set terms and conditions. Syndicated Loans ‚Äì Blockchain can reduce the time of processing in syndicated loans. Usually it takes around 19 days for the bank to settle the transactions. Also if in case the load is settled out early then also the processing time is huge. It takes time in communication as the still fax is used in most cases. However, using blockchain this will not be the case as it will connect the owners in a way that one update is reflected everywhere. It is reflected across all system. 6.5.3 Vision and Goal of blockchain in Banking Industry The long term vision of using Blockchain in banking industry is that it can be ‚Äòfabric for financial services‚Äô. It will evolve as a decentralized universal database for banks. This will have uses in variety of interesting ways. New players can hook up to the chain and start bank without the heavy infrastructure. Another vision is to create a ledger that secures records and provides authentication of data. Goals of Blockchain can be following: allow digital information to be recorded and distributed, but not edited. make banking ‚Äòmiddle-man free‚Äô and hence reduce the fee and processing time. to make it ‚Äòcensorship-free‚Äô. The blockchain network is not controlled by a single party but is P2P so if someone tries to hack the network they will need to hack every node which is nearly impossible and extremely expensive. ‚Äòtrustless‚Äô - it means that in blockchain network people need not explicitly know or trust each other for the system to function. As the network is open, peers are free to join and leave without any permissions. ‚Äòfailure-free‚Äô - blockchain can provide data consistency as it is distributed leader and is not a centralized server. 6.5.4 Blockchain Architecture in Banking Industry Blockchain architecture depends on the use case. In this case, banking industry, it would be ideal to use a closed permissioned blockchain as bank needs a private network that is accessible only to those who have permission and transactions can be edited by authorized persons who form the consensus. We can use open blockchain framework, Hyperledger Fabric by Linux Foundation. This makes it easy to share information in trusted manner. As the objective in banking is to provide trusted mechanism to validate and transfer assets and record change of ownerships, we can use Proof of Work (PoW) where one can solve a complex hash puzzle to create block and that can be verified by other participants. We can also use Crash Fault Tolerance (CFT) to avoid multiple sequencing components from failing. Also, Byzantine fault tolerance (BFT) can be used to prevent against malicious nodes. Smart Contracts can be interpreted as digital agreements used in various banking services like when giving credit. In these once the terms of agreement are met, it verifies itself and transfers the tokens as per terms. It can also interact with other smart contract. Application layer provides an interface, which allows transfer of ownership of asset from one entity to another. This will abstract the complex underlying logic and will facilitate the services to end user. 6.5.5 Timeline of implementing blockchain in Banking Industry Timeline of project implementation in blockchain may include following steps: Business Use Case - As blockchain seems to be an innovation in banking industry we can begin with the identification of the business use case. We can do risk assessment and analyze the operational effectiveness. This could take about a month. Stake Holders/Committee/Regulations - Next we can incorporate the industry standards and regulatory assessments. This will involve stake holders‚Äô agreements and formation of various committees like Steering committee, Business Management Committee and Working Committee (infrastructure and technology partners). This could take about a month or two. Technology Stack - Next, we can being with the designing of blockchain technology stack, here we can define roles and location of nodes. In this we define: Infrastructure like cloud implementation, nodes and virtualization. Network and Protocols, here we define the consensus mechanism and core protocol. Services Layer, here we define the blockchain app like Hyperledger Fabric and it‚Äôs integration with other applications. Application Layer which is the UI of the app and has business logic and helps users interact. Once this is all developed and we do a POCs on this stack. Here we need to ensure that our stack maintains the privacy for allowing to store public and private transactions. It should be fault tolerant, failing node should be able to resync with network. This could take 3-4 months. Testing/Pilot Phase - Once done, we can continue the sandbox testing and extend the pilot phase with high confidence. Here stake holders and committee members need to be involved for regular testing and ‚Äòshow and tell‚Äô session to see if we meet all the regulations and standards. This could take 2-3 months. Deployment - Lastly, we need to do deployment of the product in full swing. We need to onboard the workforce and training them to use the new technology and adapt to use blocks and smart contracts for transactions and agreements respectively. This could take 5-6 months. 6.5.6 Things to Consider before using blockchain in Banking Industry We can see that blockchain is in early adoption stage and banking is a industry which needs trusted system to handover the business functionalities. Though blockchain can improve lot of legacy things in bank still we see many challenges in the road ahead: International Regulations: Use of blockchain technology may require use of cryptocurrency and new way of doing agreement like smart contracts. These need to be adapted in a wider market which is a big concern. The consensus needs to be firm which is difficult to make and it should be robust enough in the algorithm. Lack of expertise in blockchain technology: As this is still a growing technology hence market professionals are not that experienced to take the project at such high level. Market is still evolving and needs to be more robust before penetrating in banking industry with fool proof trust. Capital requirement: Eyeing the future growth and making a big investment by bank is another step that requires lot of courage and trust in the system. The system is in early building stage which make it more difficult to adapt and hence invest money in technology and the right set of talent in the market or educating the existing workforce to adapt the new technology of blockchain. Awareness in the ecosystem: Blockchain is still in POCs or hardly has been in production in any bank. Hence it is missing in the banking industry ecosystem. Banking is a industry that very interrelated in itself and with the outer would and other industries. Hence, blockchain can come altogether in the eco system at once which again seems to be a challenge. System Integration: Adaption of new technology is fruitful if it can be seamlessly integrated with the existing technology. The legacy systems will still function and they need to interact in perfect harmony with the blockchain to make the system efficient and effective. Global adaption, governance and collaboration is required to seamlessly integrate all systems together. Scalability: Banking systems have evolved over years to facilitate millions of transactions in a day. Blockchain needs to evolve to adapt this large scale of processing and make it better than what we have currently. This requires lot of research and testing and hence a challenge. Abstraction of the project: The project in blockchain which will bring in the cryptocurrency needs to be abstract so that it could be adapted globally in accordance to requirements by different institutions, this is massive in itself hence a challenge to be worked on. Nascent Technology: Blockchain is a niche technology but it is in nascent stage at the moment to be adapted by banking industry. Many consultancies across the globe have done POCs but still the face many challenges when it comes to be adapted on a global level at a large scale. This is a big challenge when it comes to adaption of technology and its use in industry as end to end technology. References: Barclays Business Insider Nelito Deloitte Reserve Bank of India "],
["numpy.html", "7 NumPy 7.1 Numpy Arrays 7.2 Differences 7.3 Functions", " 7 NumPy NumPy (or Numpy) is a Linear Algebra Library for Python, the reason it is so important for Data Science with Python is that almost all of the libraries in the PyData Ecosystem rely on NumPy as one of their main building blocks. Numpy is also incredibly fast, as it has bindings to C libraries. For more info on why you would want to use Arrays instead of lists, check out this great StackOverflow post. Important aspects of Numpy: vectors,arrays,matrices, and number generation. 7.1 Numpy Arrays Arrays are used to represent Vectors and Matrices. Vector 1-d Matrix 2-d (can be 1X1 as well) 7.1.1 arange we can build array using this. Notes on Python as I learn and improve my understanding. 7.2 Differences Any String is by default a list of characters. ##Data Structures Python‚Äôs in built data sets are list, dict and sets. list = [1,2,3,&#39;hi&#39;,[2,3,4]] # mix data type list # list is collection of objects dictionary = {&#39;k1&#39;:&#39;v1&#39; , &#39;k2&#39;:&#39;v2&#39;} # Key value pair tuple = (1,2,3) # tuples are fixed values and cannot be assigned set = {1,2,3,1,2,1,2,1,2,1,2,1,3} # set can have only unique values # for above result is only 1,2,3 All above can be accessed using [] and index. index from 0 and end not included [0:3] gives 0,1,2 and not 3 [0:-2] all but last two. List Comprehension Defines list in one line, for eg, to find squares sqr = [x**2 for x in range(5)] This gives list with squares of numbers 0 to 4 7.3 Functions Can have defaults def my_square(num): &quot;&quot;&quot; This is DocString Can be multiline THis func squares a number &quot;&quot;&quot; return num**2 "],
["python.html", "8 Python 8.1 Web Dev Libs", " 8 Python 8.0.1 Map and Filter Map is used to apply a function to a list. seq = [1,2,3,4,5] map(my_square,seq) It computes my_square for all items in seq. Lambda This is used to define a function in a line. lambda num: num*2 That means passing variable : return statement. Useful for use in map. 8.0.1.1 Filters Filter is used to filter items in seq based on function/lambda eg: to get even values filter(lambda num: num%2 == 0,seq) 8.1 Web Dev Libs Flask light weight web framework FlaskRESTful - extention for REST framework sqlite3 store to local db Shelve Persistant object storage library Pickle Dump objects to file and load back Pandas Data maipulation Pandas vs Dask vs Spark less than a gb, use pandas upto 100 gb, use pandas with chunk or dask or pyspark more than 100gb, pyspark for sure, upto peta bytes Pandas, Dask or PySpark - Medium Superfast Pandas, make data transformation in pandas upto 7237 times faster: use iterrow use apply(lambda) use pandas vectorization, loc(filters here) use numpy vectorization, pass as numpy.array, converts to C code. Making pandas loop faster "],
["plotting-in-bookdown.html", "Plotting in BookDown 8.2 Plotly 8.3 Seaborn", " Plotting in BookDown This article shows how to plot charts in rmd using the fllowing libraries. 8.2 Plotly 8.3 Seaborn import matplotlib.pyplot as plt import seaborn as sns sns.set(style=&quot;ticks&quot;) # Load the example dataset for Anscombe&#39;s quartet df = sns.load_dataset(&quot;anscombe&quot;) # Show the results of a linear regression within each dataset sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, col=&quot;dataset&quot;, hue=&quot;dataset&quot;, data=df, col_wrap=2, ci=None, palette=&quot;muted&quot;, height=4, scatter_kws={&quot;s&quot;: 50, &quot;alpha&quot;: 1}) plt.show() "]
]
